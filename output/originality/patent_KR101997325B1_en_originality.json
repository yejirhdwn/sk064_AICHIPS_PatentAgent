{
  "target_patent_id": "patent/KR101997325B1/en",
  "originality_score": 0.9937739838671784,
  "statistics": {
    "base_cpc_count": 18,
    "expanded_cpc_count": 208,
    "total_cpc_count": 226,
    "unique_cpc_count": 192,
    "citations_analyzed": 2,
    "patents_expanded": 15,
    "api_calls_saved": "~11 calls (vs original)"
  },
  "cpc_distribution": {
    "G06F12/121": 1,
    "G06F12/0893": 2,
    "G06F12/0888": 2,
    "G06F2212/1016": 1,
    "G06F2212/502": 1,
    "G06F12/0842": 2,
    "G06F12/0811": 3,
    "G06F12/0813": 1,
    "G06F12/0833": 1,
    "G06F12/109": 1,
    "G06F9/45558": 1,
    "G06F2009/45595": 1,
    "G06F2212/1021": 1,
    "G06F2212/152": 1,
    "G06F2212/283": 2,
    "G06F2212/62": 2,
    "Y02D10/00": 3,
    "G06F9/30021": 1,
    "G06F11/348": 1,
    "G06F11/3024": 1,
    "G06F11/3409": 1,
    "G06F15/17362": 1,
    "G06F15/17381": 1,
    "G06F15/17393": 1,
    "G06F9/3001": 2,
    "G06F9/30018": 1,
    "G06F9/30145": 1,
    "H04L67/10": 1,
    "G06F2201/88": 1,
    "G06F9/5044": 1,
    "G06F13/1678": 1,
    "G06K9/00979": 1,
    "G06N3/045": 5,
    "G06N3/0464": 3,
    "G06N3/048": 1,
    "G06N3/06": 1,
    "G06N3/063": 2,
    "G06N3/084": 2,
    "G06V10/82": 1,
    "G06V10/95": 1,
    "G06N3/02": 1,
    "Y02D10/14": 1,
    "G06F12/0846": 1,
    "G06F12/0875": 3,
    "G06F13/1668": 1,
    "G06F9/5027": 1,
    "G06F2212/1024": 1,
    "G06F2212/603": 1,
    "G06N3/044": 2,
    "G06N3/0445": 1,
    "G06N3/0454": 1,
    "G06N20/20": 2,
    "G06N3/08": 1,
    "G06N3/006": 1,
    "G06N3/0442": 1,
    "G06N3/09": 2,
    "G06N3/092": 1,
    "G06N3/098": 1,
    "G06N5/022": 2,
    "G06N3/126": 1,
    "G06N5/01": 1,
    "G06N7/01": 3,
    "G06F21/75": 1,
    "G06F21/14": 1,
    "G06N20/00": 2,
    "G06N3/0475": 1,
    "G06N3/094": 1,
    "G06N3/10": 1,
    "G06N5/04": 2,
    "G06F16/24573": 1,
    "G06F16/248": 1,
    "G06F16/9024": 2,
    "G06F16/906": 1,
    "G06F40/205": 2,
    "G06F40/30": 1,
    "G06F40/216": 1,
    "G06F40/284": 1,
    "G06F16/22": 1,
    "G06F16/31": 1,
    "G06F9/505": 3,
    "G06F2209/5017": 2,
    "G06F18/2148": 1,
    "G06F18/2431": 1,
    "G06F9/3877": 1,
    "G06N3/04": 1,
    "G06N3/0499": 1,
    "G16B20/00": 1,
    "G16B20/20": 1,
    "G16B40/00": 1,
    "G16B40/20": 1,
    "H03M13/00": 1,
    "G06F15/7867": 2,
    "G06N5/025": 1,
    "G06F16/24544": 1,
    "G06F16/24568": 1,
    "G06F9/542": 1,
    "G06Q10/10": 1,
    "G06F11/1008": 1,
    "G06F12/084": 1,
    "G06F15/80": 1,
    "G06F7/00": 1,
    "G06F9/30036": 1,
    "G06F9/30043": 1,
    "G06F9/3012": 1,
    "G06F9/30123": 1,
    "G06F9/3017": 1,
    "G06F9/30181": 1,
    "G06F9/345": 1,
    "G06F9/3824": 1,
    "G06F9/3826": 1,
    "G06F9/3834": 1,
    "G06F9/3851": 1,
    "G06F9/3865": 1,
    "G06F9/3891": 1,
    "G06F9/526": 1,
    "G06T1/20": 1,
    "G06T1/60": 1,
    "G06F2212/452": 1,
    "G06F9/5083": 1,
    "G06F9/44505": 1,
    "G06F9/485": 1,
    "G06F3/0605": 1,
    "G06F16/122": 1,
    "G06F16/1748": 1,
    "G06F16/1827": 1,
    "G06F16/1844": 1,
    "G06F16/41": 1,
    "G06F3/061": 1,
    "G06F3/0626": 1,
    "G06F3/0631": 1,
    "G06F3/0641": 1,
    "G06F3/0649": 1,
    "G06F3/0667": 1,
    "G06F3/067": 1,
    "G06Q30/02": 1,
    "G06Q30/0206": 1,
    "G06Q50/188": 1,
    "H04L63/0428": 1,
    "H04L67/1095": 1,
    "H04L67/1097": 1,
    "H04L67/56": 1,
    "H04L67/5682": 1,
    "G06F11/3485": 1,
    "G06F3/06": 1,
    "H04L67/02": 1,
    "H04L67/06": 1,
    "H04L67/535": 1,
    "H04L69/08": 1,
    "Y04S40/20": 1,
    "G06F16/24578": 1,
    "G06F16/254": 1,
    "G06F16/285": 1,
    "G06F16/444": 1,
    "G06F17/2235": 1,
    "G06F17/30061": 1,
    "G06F17/3053": 1,
    "G06F17/30563": 1,
    "G06F17/30598": 1,
    "G06F17/30958": 1,
    "G06F3/0482": 1,
    "G06F3/0484": 1,
    "G06F3/04842": 1,
    "G06F3/04847": 1,
    "G06F40/134": 1,
    "G06K9/2063": 1,
    "G06N7/005": 1,
    "G06N99/005": 1,
    "G06V10/225": 1,
    "H04L41/0893": 1,
    "H04L41/145": 1,
    "H04L41/22": 1,
    "H04L43/00": 1,
    "H04L43/045": 1,
    "G06F9/544": 1,
    "B29C49/062": 1,
    "B29C49/18": 1,
    "B29C49/42122": 1,
    "B29C49/4215": 1,
    "B29D22/003": 1,
    "B29C2049/023": 1,
    "B29C49/42087": 1,
    "B29C49/6427": 1,
    "G06F9/466": 1,
    "G06F16/24532": 1,
    "G06F16/24542": 1,
    "G06F17/2705": 1,
    "G06F9/4843": 1,
    "G06F9/5038": 1,
    "G06F9/5066": 1,
    "G06F2209/5018": 1,
    "G06F2209/504": 1,
    "Y02D10/22": 1
  },
  "citation_patents": [
    {
      "title": "System and Method for Effective Caching Using Neural Networks",
      "abstract": "Systems and methods for selecting an appropriate caching algorithm to be used when temporarily storing data accessed by an executing application using a neural network may dynamically and/or iteratively replace an initial caching algorithm being used for the application. An input layer of the neural network may gather values of performance related parameters, such as cache hit rates, data throughput rates, or memory access request response times. The neural network may detect a pattern or change in a pattern of accesses, or a change in a workload, a hardware component, or an operating system parameter. Dependent on these and/or other inputs, the neural network may select and apply a caching algorithm likely to improve performance of the application. Other inputs to the neural network may include values of hardware configuration parameters and/or operating system parameters. The neural network may perform a training exercise or may be self-training, e.g., using reinforcement learning.",
      "patent_id": null,
      "publication_number": "US20120041914A1",
      "publication_date": "2012-02-16",
      "filing_date": "2010-08-16",
      "priority_date": "2010-08-16",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/1c/a9/d2/7c81a0d15f1286/US20120041914A1.pdf"
    },
    {
      "title": "Hardware/software co-optimization to improve performance and energy for inter-vm communication for nfvs and other producer-consumer workloads",
      "abstract": "Methods and apparatus implementing Hardware/Software co-optimization to improve performance and energy for inter-VM communication for NFVs and other producer-consumer workloads. The apparatus include multi-core processors with multi-level cache hierarchies including and L1 and L2 cache for each core and a shared last-level cache (LLC). One or more machine-level instructions are provided for proactively demoting cachelines from lower cache levels to higher cache levels, including demoting cachelines from L1/L2 caches to an LLC. Techniques are also provided for implementing hardware/software co-optimization in multi-socket NUMA architecture system, wherein cachelines may be selectively demoted and pushed to an LLC in a remote socket. In addition, techniques are disclosure for implementing early snooping in multi-socket systems to reduce latency when accessing cachelines on remote sockets.",
      "patent_id": null,
      "publication_number": "US20160188474A1",
      "publication_date": "2016-06-30",
      "filing_date": "2014-12-26",
      "priority_date": "2014-12-26",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/8e/75/98/d8325ce5d10c8f/US20160188474A1.pdf"
    }
  ],
  "expanded_patents": [
    {
      "title": "Opcode counting for performance measurement",
      "abstract": "Methods, systems and computer program products are disclosed for measuring a performance of a program running on a processing unit of a processing system. In one embodiment, the method comprises informing a logic unit of each instruction in the program that is executed by the processing unit, assigning a weight to each instruction, assigning the instructions to a plurality of groups, and analyzing the plurality of groups to measure one or more metrics. In one embodiment, each instruction includes an operating code portion, and the assigning includes assigning the instructions to the groups based on the operating code portions of the instructions. In an embodiment, each type of instruction is assigned to a respective one of the plurality of groups. These groups may be combined into a plurality of sets of the groups.",
      "patent_id": null,
      "publication_number": "US10713043B2",
      "publication_date": "2020-07-14",
      "filing_date": "2018-03-12",
      "priority_date": "2010-01-08",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/49/3f/0e/47b94ce4e0fa82/US10713043.pdf"
    },
    {
      "title": "Systems and methods for data management",
      "abstract": "A method for data management is provided. The method comprises: storing the plurality of items in a contiguous space within the memory, executing an instruction containing an address and a size that together identify the contiguous space to transmit the plurality of items from the main memory to a random-access memory (RAM) on a chip, and the chip includes a computing unit comprising a plurality of multipliers; and instructing the computing unit on the chip to: retrieve multiple of the plurality of items from the RAM; and perform a plurality of parallel operations using the plurality of multipliers with the multiple items to yield output data.",
      "patent_id": null,
      "publication_number": "US10241837B2",
      "publication_date": "2019-03-26",
      "filing_date": "2018-02-05",
      "priority_date": "2016-12-09",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/65/8d/85/e5f4dda57c8849/US10241837.pdf"
    },
    {
      "title": "Accelerator comprising input and output controllers for feeding back intermediate data between processing elements via cache module",
      "abstract": "An accelerator and a system for accelerating operations are disclosed. A respective apparatus comprises an interface configured to couple the apparatus to an interconnect, a plurality of processing modules, each processing module configured to process data, a control module configured to control processing of each of the plurality of processing modules, and a cache module configured to store at least a portion of data processed by at least one of the plurality of processing modules. Each processing module includes a processing core configured to process data by performing an operation on the data using a plurality of processing elements, an input control unit configured to retrieve data via the interface and data stored in the cache module and to provide the retrieved data to the processing core, and an output control unit configured to provide data processed by the processing core to the interface and the cache module.",
      "patent_id": null,
      "publication_number": "US11467969B2",
      "publication_date": "2022-10-11",
      "filing_date": "2019-04-09",
      "priority_date": "2018-04-19",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/a2/5b/b0/233aebd12e14e4/US11467969.pdf"
    },
    {
      "title": "Machine-learning models to leverage behavior-dependent processes",
      "abstract": "Provided is a process, including: obtaining a first training dataset of subject-entity records; training a first machine-learning model on the first training dataset; forming virtual subject-entity records by appending members of a set of candidate action sequences to time-series of at least some of the subject-entity records; forming a second training dataset by labeling the virtual subject-entity records with predictions of the first machine-learning model; and training a second machine-learning model on the second training dataset.",
      "patent_id": null,
      "publication_number": "US20230186083A1",
      "publication_date": "2023-06-15",
      "filing_date": "2022-11-15",
      "priority_date": "2018-09-11",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/3d/4c/a4/352d496fcfbf30/US20230186083A1.pdf"
    },
    {
      "title": "Auditable secure reverse engineering proof machine learning pipeline and methods",
      "abstract": "Provided is a process including: searching code of a machine-learning pipeline to find a first and a second object code sequences performing similar tasks; modifying the code of the machine learning pipeline by inserting a third object code sequence into the code of the machine learning pipeline, the third code sequence being operable to pass control to the first object code sequence; inserting a branch at the end of the first code sequence, the branch being operable to: pass control, upon detection of a first predefined condition, to an instruction following the first object code sequence, and to pass control, upon detection of a second predefined condition, to an instruction following the third object code sequence; and wherein the third code sequence is executed in place of the second object sequence without affecting completion of the tasks.",
      "patent_id": null,
      "publication_number": "US20210342490A1",
      "publication_date": "2021-11-04",
      "filing_date": "2021-05-04",
      "priority_date": "2020-05-04",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/6f/08/6d/0a5420f674c0b4/US20210342490A1.pdf"
    },
    {
      "title": "Platform for semantic search and dynamic reclassification",
      "abstract": "A platform receives an input document from a user device and automatically determines a semantic signature for the input document based on a probabilistic distribution of rare words within the input document. The platform automatically scrapes at least one Internet database for additional documents and webpages, determining semantic signatures for each document or webpage. Based on similarity of semantic signatures, the platform automatically constructs and displays a graphical network of documents, wherein each document is represented as a node and similarity of semantic signatures is used to determine the locations of edges between nodes. The graph automatically groups nodes by communities and selects nodes in different communities to promote serendipity of results.",
      "patent_id": null,
      "publication_number": "US12061612B1",
      "publication_date": "2024-08-13",
      "filing_date": "2023-01-20",
      "priority_date": "2013-05-23",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/8d/6a/4d/5085032daa58fa/US12061612.pdf"
    },
    {
      "title": "Systems and methods for scalable delocalized information governance",
      "abstract": "The invention relates to electronic indexing, and more particularly, to the indexing, in a cloud, data held in a cloud. Systems and methods of the invention index data by accessing the data in place in the cloud and breaking a job into work items and sending the work items to multiple cloud processes that can each determine whether to index data associated with the work item or to create a new work item and have a different cloud process index the data. Each cloud process is proximal to an item that it indexes. This gives the system scale as well as an internal load-balancing.",
      "patent_id": null,
      "publication_number": "US20250272272A1",
      "publication_date": "2025-08-28",
      "filing_date": "2025-05-01",
      "priority_date": "2008-02-11",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": null
    },
    {
      "title": "Deep learning-based variant classifier",
      "abstract": "The technology disclosed directly operates on sequencing data and derives its own feature filters. It processes a plurality of aligned reads that span a target base position. It combines elegant encoding of the reads with a lightweight analysis to produce good recall and precision using lightweight hardware. For instance, one million training examples of target base variant sites with 50 to 100 reads each can be trained on a single GPU card in less than 10 hours with good recall and precision. A single GPU card is desirable because it a computer with a single GPU is inexpensive, almost universally within reach for users looking at genetic data. It is readily available on could-based platforms.",
      "patent_id": null,
      "publication_number": "US20250239329A1",
      "publication_date": "2025-07-24",
      "filing_date": "2024-12-26",
      "priority_date": "2018-01-15",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": null
    },
    {
      "title": "Method and system for accelerated stream processing",
      "abstract": "Disclosed herein are methods and systems for hardware-accelerating various data processing operations in a rule-based decision-making system such as a business rules engine, an event stream processor, and a complex event stream processor. Preferably, incoming data streams are checked against a plurality of rule conditions. Among the data processing operations that are hardware-accelerated include rule condition check operations, filtering operations, and path merging operations. The rule condition check operations generate rule condition check results for the processed data streams, wherein the rule condition check results are indicative of any rule conditions which have been satisfied by the data streams. The generation of such results with a low degree of latency provides enterprises with the ability to perform timely decision-making based on the data present in received data streams.",
      "patent_id": null,
      "publication_number": "US11677417B2",
      "publication_date": "2023-06-13",
      "filing_date": "2021-03-29",
      "priority_date": "2008-05-15",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/36/24/a8/7081c711b6f48d/US11677417.pdf"
    },
    {
      "title": "Multiple multithreaded processors with shared data cache",
      "abstract": "A multi-core processor configured to improve processing performance in certain computing contexts is provided. The multi-core processor includes multiple processing cores that implement barrel threading to execute multiple instruction threads in parallel while ensuring that the effects of an idle instruction or thread upon the performance of the processor is minimized. The multiple cores can also share a common data cache, thereby minimizing the need for expensive and complex mechanisms to mitigate inter-cache coherency issues. The barrel-threading can minimize the latency impacts associated with a shared data cache. In some examples, the multi-core processor can also include a serial processor configured to execute single threaded programming code that may not yield satisfactory performance in a processing environment that employs barrel threading.",
      "patent_id": null,
      "publication_number": "US20240394191A1",
      "publication_date": "2024-11-28",
      "filing_date": "2024-08-08",
      "priority_date": "2015-06-10",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/89/e9/5f/226cc80bad2c81/US20240394191A1.pdf"
    },
    {
      "title": "Acceleration method for FPGA-based distributed stream processing system",
      "abstract": "The present invention relates to an acceleration method for an FPGA-based distributed stream processing system, which accomplishes computational processing of stream processing operations through collaborative computing conducted by FPGA devices and a CPU module and at least comprises following steps: building the FPGA-based distributed stream processing system having a master node by installing the FPGA devices on slave nodes; dividing stream applications into first tasks suitable to be executed by the FPGA devices and second tasks suitable to be executed by the CPU module; and where the stream applications submitted to the master node are configured with kernel files that can be compiled and executed by the FPGA devices or with uploading paths of the kernel files, making the master node allocate and schedule resources by pre-processing the stream applications.",
      "patent_id": null,
      "publication_number": "US11023285B2",
      "publication_date": "2021-06-01",
      "filing_date": "2020-01-27",
      "priority_date": "2019-04-12",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/64/e3/0b/69d98196ab580e/US11023285.pdf"
    },
    {
      "title": "Data object store and server for a cloud storage environment",
      "abstract": "Data storage operations, including content-indexing, containerized deduplication, and policy-driven storage, are performed within a cloud environment. The systems support a variety of clients and cloud storage sites that may connect to the system in a cloud environment that requires data transfer over wide area networks, such as the Internet, which may have appreciable latency and/or packet loss, using various network protocols, including HTTP and FTP. Methods are disclosed for content indexing data stored within a cloud environment to facilitate later searching, including collaborative searching. Methods are also disclosed for performing containerized deduplication to reduce the strain on a system namespace, effectuate cost savings, etc. Methods are disclosed for identifying suitable storage locations, including suitable cloud storage sites, for data files subject to a storage policy. Further, systems and methods for providing a cloud gateway and a scalable data object store within a cloud environment are disclosed, along with other features.",
      "patent_id": null,
      "publication_number": "US12321592B2",
      "publication_date": "2025-06-03",
      "filing_date": "2024-02-05",
      "priority_date": "2009-06-30",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/f8/55/87/f2efdb0ad38949/US12321592B2.pdf"
    },
    {
      "title": "Modular model workflow in a distributed computation system",
      "abstract": "A security platform employs a variety techniques and mechanisms to detect security related anomalies and threats in a computer network environment. The security platform is “big data” driven and employs machine learning to perform security analytics. The security platform performs user/entity behavioral analytics (UEBA) to detect the security related anomalies and threats, regardless of whether such anomalies/threats were previously known. The security platform can include both real-time and batch paths/modes for detecting anomalies and threats. By visually presenting analytical results scored with risk ratings and supporting evidence, the security platform enables network security administrators to respond to a detected anomaly or threat, and to take action promptly.",
      "patent_id": null,
      "publication_number": "US10110617B2",
      "publication_date": "2018-10-23",
      "filing_date": "2015-10-30",
      "priority_date": "2015-08-31",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/27/98/23/c337faf782ba39/US10110617.pdf"
    },
    {
      "title": "Scheduler, multi-core processor system, and scheduling method",
      "abstract": "A scheduler that causes a given core in a multi-core processor to determine if a priority level of a process that is to be executed by a core of the multi-core processor is greater than or equal to a threshold; save to a cache memory of each core that executes a process having a priority level greater than or equal to the threshold, data that is accessed by the process upon execution; save to a memory area different from the cache memory and to which access is relatively slower, data that is accessed by a process having a priority level not greater than or equal to the threshold; and save the data saved in the memory area, to a cache memory of a requesting core, when the requesting core issues an access request for the data saved in the memory area.",
      "patent_id": null,
      "publication_number": "US9430388B2",
      "publication_date": "2016-08-30",
      "filing_date": "2015-01-21",
      "priority_date": "2010-08-27",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/4b/ca/d2/792cb50dd11601/US9430388.pdf"
    },
    {
      "title": "Task scheduling for highly concurrent analytical and transaction workloads",
      "abstract": "Systems and method for a task scheduler with dynamic adjustment of concurrency levels and task granularity are disclosed for improved execution of highly concurrent analytical and transactional systems. The task scheduler can avoid both over commitment and underutilization of computing resources by monitoring and controlling the number of active worker threads. The number of active worker threads can be adapted to avoid underutilization of computing resources by giving the OS control of additional worker threads processing blocked application tasks. The task scheduler can dynamically determine a number of parallel operations for a particular task based on the number of available threads. The number of available worker threads can be determined based on the average availability of worker threads in the recent history of the application. Based on the number of available worker threads, the partitionable operation can be partitioned into a number of sub operations and executed in parallel.",
      "patent_id": null,
      "publication_number": "US10545789B2",
      "publication_date": "2020-01-28",
      "filing_date": "2018-04-25",
      "priority_date": "2013-06-24",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/59/98/6a/e52ceee7a4d149/US10545789.pdf"
    }
  ]
}