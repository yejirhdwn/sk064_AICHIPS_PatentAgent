{
  "target_patent_id": "patent/KR101997325B1/en",
  "originality_score": 0.9920317362607298,
  "statistics": {
    "base_cpc_count": 18,
    "expanded_cpc_count": 153,
    "total_cpc_count": 171,
    "unique_cpc_count": 148,
    "citations_analyzed": 2,
    "patents_expanded": 15,
    "api_calls_saved": "~11 calls (vs original)"
  },
  "cpc_distribution": {
    "G06F12/121": 1,
    "G06F12/0893": 2,
    "G06F12/0888": 1,
    "G06F2212/1016": 2,
    "G06F2212/502": 1,
    "G06F12/0842": 2,
    "G06F12/0811": 2,
    "G06F12/0813": 1,
    "G06F12/0833": 1,
    "G06F12/109": 2,
    "G06F9/45558": 2,
    "G06F2009/45595": 1,
    "G06F2212/1021": 1,
    "G06F2212/152": 1,
    "G06F2212/283": 1,
    "G06F2212/62": 2,
    "Y02D10/00": 3,
    "G06F9/30021": 1,
    "G06F11/348": 1,
    "G06F11/3024": 1,
    "G06F11/3409": 1,
    "G06F15/17362": 1,
    "G06F15/17381": 1,
    "G06F15/17393": 1,
    "G06F9/3001": 2,
    "G06F9/30018": 1,
    "G06F9/30145": 1,
    "H04L67/10": 1,
    "G06F2201/88": 1,
    "G06F9/5044": 1,
    "G06F13/1678": 1,
    "G06K9/00979": 1,
    "G06N3/045": 5,
    "G06N3/0464": 3,
    "G06N3/048": 1,
    "G06N3/06": 1,
    "G06N3/063": 2,
    "G06N3/084": 2,
    "G06V10/82": 1,
    "G06V10/95": 1,
    "G06N3/02": 1,
    "Y02D10/14": 1,
    "G06F12/0846": 1,
    "G06F12/0875": 2,
    "G06F13/1668": 1,
    "G06F9/5027": 1,
    "G06F2212/1024": 1,
    "G06F2212/603": 1,
    "G06N3/044": 2,
    "G06N3/0445": 1,
    "G06N3/0454": 1,
    "G06N20/20": 1,
    "G06N3/08": 1,
    "G06N3/006": 1,
    "G06N3/0442": 1,
    "G06N3/09": 2,
    "G06N3/092": 1,
    "G06N3/098": 1,
    "G06N5/022": 1,
    "G06N3/126": 1,
    "G06N5/01": 1,
    "G06N7/01": 2,
    "G06F21/75": 1,
    "G06F21/14": 1,
    "G06N20/00": 1,
    "G06N3/0475": 1,
    "G06N3/094": 1,
    "G06N3/10": 1,
    "G06N5/04": 1,
    "G06F16/24573": 1,
    "G06F16/248": 1,
    "G06F16/9024": 1,
    "G06F16/906": 1,
    "G06F40/205": 1,
    "G06F40/30": 1,
    "G06F40/216": 1,
    "G06F40/284": 1,
    "G06F16/22": 1,
    "G06F16/31": 1,
    "G06F9/505": 1,
    "G06F2209/5017": 1,
    "G06F18/2148": 1,
    "G06F18/2431": 1,
    "G06F9/3877": 1,
    "G06N3/04": 1,
    "G06N3/0499": 1,
    "G16B20/00": 1,
    "G16B20/20": 1,
    "G16B40/00": 1,
    "G16B40/20": 1,
    "H03M13/00": 1,
    "G06F15/7867": 2,
    "G06N5/025": 1,
    "G06F16/24544": 1,
    "G06F16/24568": 1,
    "G06F9/542": 1,
    "G06Q10/10": 1,
    "G06F11/1008": 1,
    "G06F12/084": 1,
    "G06F15/80": 1,
    "G06F7/00": 1,
    "G06F9/30036": 1,
    "G06F9/30043": 1,
    "G06F9/3012": 1,
    "G06F9/30123": 1,
    "G06F9/3017": 1,
    "G06F9/30181": 1,
    "G06F9/345": 1,
    "G06F9/3824": 1,
    "G06F9/3826": 1,
    "G06F9/3834": 1,
    "G06F9/3851": 1,
    "G06F9/3865": 1,
    "G06F9/3891": 1,
    "G06F9/526": 1,
    "G06T1/20": 1,
    "G06T1/60": 1,
    "G06F2212/452": 1,
    "G06F12/1009": 1,
    "G06F2212/202": 1,
    "G06F9/45537": 1,
    "G06F1/3275": 1,
    "G06F11/0712": 1,
    "G06F11/073": 1,
    "G06F12/0882": 1,
    "G06F12/12": 1,
    "G06F11/0793": 1,
    "Y02D30/50": 1,
    "G06F12/08": 1,
    "G06F12/023": 1,
    "G06F12/0253": 1,
    "G06F2009/45583": 1,
    "G06F2212/151": 1,
    "G06F3/0604": 1,
    "G06F12/0223": 1,
    "G06F12/0246": 1,
    "G06F12/06": 1,
    "G06F3/061": 1,
    "G06F3/0631": 1,
    "G06F3/0638": 1,
    "G06F3/0653": 1,
    "G06F3/0679": 1,
    "G06F12/00": 1,
    "G06F2212/1036": 1,
    "G06F2212/205": 1,
    "G06F2212/7201": 1,
    "G06F2212/7202": 1,
    "G06F2212/7208": 1
  },
  "citation_patents": [
    {
      "title": "System and Method for Effective Caching Using Neural Networks",
      "abstract": "Systems and methods for selecting an appropriate caching algorithm to be used when temporarily storing data accessed by an executing application using a neural network may dynamically and/or iteratively replace an initial caching algorithm being used for the application. An input layer of the neural network may gather values of performance related parameters, such as cache hit rates, data throughput rates, or memory access request response times. The neural network may detect a pattern or change in a pattern of accesses, or a change in a workload, a hardware component, or an operating system parameter. Dependent on these and/or other inputs, the neural network may select and apply a caching algorithm likely to improve performance of the application. Other inputs to the neural network may include values of hardware configuration parameters and/or operating system parameters. The neural network may perform a training exercise or may be self-training, e.g., using reinforcement learning.",
      "patent_id": null,
      "publication_number": "US20120041914A1",
      "publication_date": "2012-02-16",
      "filing_date": "2010-08-16",
      "priority_date": "2010-08-16",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/1c/a9/d2/7c81a0d15f1286/US20120041914A1.pdf"
    },
    {
      "title": "Hardware/software co-optimization to improve performance and energy for inter-vm communication for nfvs and other producer-consumer workloads",
      "abstract": "Methods and apparatus implementing Hardware/Software co-optimization to improve performance and energy for inter-VM communication for NFVs and other producer-consumer workloads. The apparatus include multi-core processors with multi-level cache hierarchies including and L1 and L2 cache for each core and a shared last-level cache (LLC). One or more machine-level instructions are provided for proactively demoting cachelines from lower cache levels to higher cache levels, including demoting cachelines from L1/L2 caches to an LLC. Techniques are also provided for implementing hardware/software co-optimization in multi-socket NUMA architecture system, wherein cachelines may be selectively demoted and pushed to an LLC in a remote socket. In addition, techniques are disclosure for implementing early snooping in multi-socket systems to reduce latency when accessing cachelines on remote sockets.",
      "patent_id": null,
      "publication_number": "US20160188474A1",
      "publication_date": "2016-06-30",
      "filing_date": "2014-12-26",
      "priority_date": "2014-12-26",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/8e/75/98/d8325ce5d10c8f/US20160188474A1.pdf"
    }
  ],
  "expanded_patents": [
    {
      "title": "Opcode counting for performance measurement",
      "abstract": "Methods, systems and computer program products are disclosed for measuring a performance of a program running on a processing unit of a processing system. In one embodiment, the method comprises informing a logic unit of each instruction in the program that is executed by the processing unit, assigning a weight to each instruction, assigning the instructions to a plurality of groups, and analyzing the plurality of groups to measure one or more metrics. In one embodiment, each instruction includes an operating code portion, and the assigning includes assigning the instructions to the groups based on the operating code portions of the instructions. In an embodiment, each type of instruction is assigned to a respective one of the plurality of groups. These groups may be combined into a plurality of sets of the groups.",
      "patent_id": null,
      "publication_number": "US10713043B2",
      "publication_date": "2020-07-14",
      "filing_date": "2018-03-12",
      "priority_date": "2010-01-08",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/49/3f/0e/47b94ce4e0fa82/US10713043.pdf"
    },
    {
      "title": "Systems and methods for data management",
      "abstract": "A method for data management is provided. The method comprises: storing the plurality of items in a contiguous space within the memory, executing an instruction containing an address and a size that together identify the contiguous space to transmit the plurality of items from the main memory to a random-access memory (RAM) on a chip, and the chip includes a computing unit comprising a plurality of multipliers; and instructing the computing unit on the chip to: retrieve multiple of the plurality of items from the RAM; and perform a plurality of parallel operations using the plurality of multipliers with the multiple items to yield output data.",
      "patent_id": null,
      "publication_number": "US10241837B2",
      "publication_date": "2019-03-26",
      "filing_date": "2018-02-05",
      "priority_date": "2016-12-09",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/65/8d/85/e5f4dda57c8849/US10241837.pdf"
    },
    {
      "title": "Accelerator comprising input and output controllers for feeding back intermediate data between processing elements via cache module",
      "abstract": "An accelerator and a system for accelerating operations are disclosed. A respective apparatus comprises an interface configured to couple the apparatus to an interconnect, a plurality of processing modules, each processing module configured to process data, a control module configured to control processing of each of the plurality of processing modules, and a cache module configured to store at least a portion of data processed by at least one of the plurality of processing modules. Each processing module includes a processing core configured to process data by performing an operation on the data using a plurality of processing elements, an input control unit configured to retrieve data via the interface and data stored in the cache module and to provide the retrieved data to the processing core, and an output control unit configured to provide data processed by the processing core to the interface and the cache module.",
      "patent_id": null,
      "publication_number": "US11467969B2",
      "publication_date": "2022-10-11",
      "filing_date": "2019-04-09",
      "priority_date": "2018-04-19",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/a2/5b/b0/233aebd12e14e4/US11467969.pdf"
    },
    {
      "title": "Machine-learning models to leverage behavior-dependent processes",
      "abstract": "Provided is a process, including: obtaining a first training dataset of subject-entity records; training a first machine-learning model on the first training dataset; forming virtual subject-entity records by appending members of a set of candidate action sequences to time-series of at least some of the subject-entity records; forming a second training dataset by labeling the virtual subject-entity records with predictions of the first machine-learning model; and training a second machine-learning model on the second training dataset.",
      "patent_id": null,
      "publication_number": "US20230186083A1",
      "publication_date": "2023-06-15",
      "filing_date": "2022-11-15",
      "priority_date": "2018-09-11",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/3d/4c/a4/352d496fcfbf30/US20230186083A1.pdf"
    },
    {
      "title": "Auditable secure reverse engineering proof machine learning pipeline and methods",
      "abstract": "Provided is a process including: searching code of a machine-learning pipeline to find a first and a second object code sequences performing similar tasks; modifying the code of the machine learning pipeline by inserting a third object code sequence into the code of the machine learning pipeline, the third code sequence being operable to pass control to the first object code sequence; inserting a branch at the end of the first code sequence, the branch being operable to: pass control, upon detection of a first predefined condition, to an instruction following the first object code sequence, and to pass control, upon detection of a second predefined condition, to an instruction following the third object code sequence; and wherein the third code sequence is executed in place of the second object sequence without affecting completion of the tasks.",
      "patent_id": null,
      "publication_number": "US20210342490A1",
      "publication_date": "2021-11-04",
      "filing_date": "2021-05-04",
      "priority_date": "2020-05-04",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/6f/08/6d/0a5420f674c0b4/US20210342490A1.pdf"
    },
    {
      "title": "Platform for semantic search and dynamic reclassification",
      "abstract": "A platform receives an input document from a user device and automatically determines a semantic signature for the input document based on a probabilistic distribution of rare words within the input document. The platform automatically scrapes at least one Internet database for additional documents and webpages, determining semantic signatures for each document or webpage. Based on similarity of semantic signatures, the platform automatically constructs and displays a graphical network of documents, wherein each document is represented as a node and similarity of semantic signatures is used to determine the locations of edges between nodes. The graph automatically groups nodes by communities and selects nodes in different communities to promote serendipity of results.",
      "patent_id": null,
      "publication_number": "US12061612B1",
      "publication_date": "2024-08-13",
      "filing_date": "2023-01-20",
      "priority_date": "2013-05-23",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/8d/6a/4d/5085032daa58fa/US12061612.pdf"
    },
    {
      "title": "Systems and methods for scalable delocalized information governance",
      "abstract": "The invention relates to electronic indexing, and more particularly, to the indexing, in a cloud, data held in a cloud. Systems and methods of the invention index data by accessing the data in place in the cloud and breaking a job into work items and sending the work items to multiple cloud processes that can each determine whether to index data associated with the work item or to create a new work item and have a different cloud process index the data. Each cloud process is proximal to an item that it indexes. This gives the system scale as well as an internal load-balancing.",
      "patent_id": null,
      "publication_number": "US20250272272A1",
      "publication_date": "2025-08-28",
      "filing_date": "2025-05-01",
      "priority_date": "2008-02-11",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": null
    },
    {
      "title": "Deep learning-based variant classifier",
      "abstract": "The technology disclosed directly operates on sequencing data and derives its own feature filters. It processes a plurality of aligned reads that span a target base position. It combines elegant encoding of the reads with a lightweight analysis to produce good recall and precision using lightweight hardware. For instance, one million training examples of target base variant sites with 50 to 100 reads each can be trained on a single GPU card in less than 10 hours with good recall and precision. A single GPU card is desirable because it a computer with a single GPU is inexpensive, almost universally within reach for users looking at genetic data. It is readily available on could-based platforms.",
      "patent_id": null,
      "publication_number": "US20250239329A1",
      "publication_date": "2025-07-24",
      "filing_date": "2024-12-26",
      "priority_date": "2018-01-15",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": null
    },
    {
      "title": "Method and system for accelerated stream processing",
      "abstract": "Disclosed herein are methods and systems for hardware-accelerating various data processing operations in a rule-based decision-making system such as a business rules engine, an event stream processor, and a complex event stream processor. Preferably, incoming data streams are checked against a plurality of rule conditions. Among the data processing operations that are hardware-accelerated include rule condition check operations, filtering operations, and path merging operations. The rule condition check operations generate rule condition check results for the processed data streams, wherein the rule condition check results are indicative of any rule conditions which have been satisfied by the data streams. The generation of such results with a low degree of latency provides enterprises with the ability to perform timely decision-making based on the data present in received data streams.",
      "patent_id": null,
      "publication_number": "US11677417B2",
      "publication_date": "2023-06-13",
      "filing_date": "2021-03-29",
      "priority_date": "2008-05-15",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/36/24/a8/7081c711b6f48d/US11677417.pdf"
    },
    {
      "title": "Multiple multithreaded processors with shared data cache",
      "abstract": "A multi-core processor configured to improve processing performance in certain computing contexts is provided. The multi-core processor includes multiple processing cores that implement barrel threading to execute multiple instruction threads in parallel while ensuring that the effects of an idle instruction or thread upon the performance of the processor is minimized. The multiple cores can also share a common data cache, thereby minimizing the need for expensive and complex mechanisms to mitigate inter-cache coherency issues. The barrel-threading can minimize the latency impacts associated with a shared data cache. In some examples, the multi-core processor can also include a serial processor configured to execute single threaded programming code that may not yield satisfactory performance in a processing environment that employs barrel threading.",
      "patent_id": null,
      "publication_number": "US20240394191A1",
      "publication_date": "2024-11-28",
      "filing_date": "2024-08-08",
      "priority_date": "2015-06-10",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/89/e9/5f/226cc80bad2c81/US20240394191A1.pdf"
    },
    {
      "title": "Virtual memory system, virtual memory controlling method, and program",
      "abstract": "Disclosed herein is a virtual memory system including a nonvolatile memory allowing random access, having an upper limit to a number of times of rewriting, and including a physical address space accessed via a virtual address; and a virtual memory control section configured to manage the physical address space of the nonvolatile memory in page units, map the physical address space and a virtual address space, and convert an accessed virtual address into a physical address; wherein the virtual memory control section is configured to expand a physical memory capacity allocated to a virtual page in which rewriting occurs.",
      "patent_id": null,
      "publication_number": "US9514056B2",
      "publication_date": "2016-12-06",
      "filing_date": "2012-02-14",
      "priority_date": "2011-03-04",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/3e/5c/d5/00e018c2e46aa5/US9514056.pdf"
    },
    {
      "title": "Machine memory power and availability management in a processing system supporting multiple virtual machines",
      "abstract": "A processing system and computer program provides memory power management and memory failure management in large scale systems. Upon a decision to take a memory module off-line or place the module in an increased-latency state for power management, or upon a notification that a memory module has failed or been taken off-line or has had latency increased by another power management control mechanism, a hypervisor that supports multiple virtual machines checks the use of pages by each virtual machine and its guest operating system by using a reverse mapping. The hypervisor determines which virtual machines are using a particular machine memory page and may re-map the machine memory page to another available machine page, or may notify the virtual machines that the memory page has become or is becoming unavailable via a fault or other notification mechanism. Alternatively, or in the absence of a response from a virtual machine, the hypervisor can shut down the affected partition(s).",
      "patent_id": null,
      "publication_number": "US7539841B2",
      "publication_date": "2009-05-26",
      "filing_date": "2008-02-13",
      "priority_date": "2003-12-17",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/73/e0/0f/5753d52860558d/US7539841.pdf"
    },
    {
      "title": "Memory management method for coupled memory multiprocessor systems",
      "abstract": "A method of managing the memory of a CM multiprocessor computer system is disclosed. A CM multiprocessor computer system includes: a plurality of CPU modules 11a . . . 11n to which processes are assigned; one or more optional global memories 13a . . . 13n; a storage medium 15a, 15b . . . 15n; and a global interconnect 12. Each of the CPU modules 11a . . . 11n includes a processor 21 and a coupled memory 23 accessible by the local processor without using the global interconnect 12. Processors have access to remote coupled memory regions via the global interconnect 12. Memory is managed by transferring, from said storage medium, the data and stack pages of a process to be run to the coupled memory region of the CPU module to which the process is assigned, when the pages are called for by the process. Other pages are transferred to global memory, if available. At prescribed intervals, the free memory of each coupled memory region and global memory is evaluated to determine if it is below a threshold. If below the threshold, a predetermined number of pages of the memory region are scanned. Infrequently used pages are placed on the end of a list of pages that can be replaced with pages stored in the storage medium. Pages associated with processes that are terminating are placed at the head of the list of replacement pages.",
      "patent_id": null,
      "publication_number": "US5237673A",
      "publication_date": "1993-08-17",
      "filing_date": "1991-03-20",
      "priority_date": "1991-03-20",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/f7/77/94/e5b860a1447cdb/US5237673.pdf"
    },
    {
      "title": "Proactive memory reclamation for java virtual machines",
      "abstract": "A mechanism is provided for managing memory of a virtual machine having a runtime environment executing therein. The runtime environment includes a balloon agent that allocates memory objects within heap memory of the runtime environment and hints to a hypervisor that machine memory pages backing the memory objects may be candidates for page sharing. At launch of the runtime environment, the balloon agent allocates memory objects in response to detecting a state of high machine memory consumption by the virtual machine. Further, while the runtime environment is running, the balloon agent allocates memory objects within heap memory when the runtime environment becomes idle.",
      "patent_id": null,
      "publication_number": "US9940228B2",
      "publication_date": "2018-04-10",
      "filing_date": "2012-06-14",
      "priority_date": "2012-06-14",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/dc/6b/0e/211e7ae93baa3d/US9940228.pdf"
    },
    {
      "title": "Memory management device predicting an erase count",
      "abstract": "A memory management device of an example of the invention controls writing into and reading from a main memory including a nonvolatile semiconductor memory and a volatile semiconductor memory in response to a writing request and a reading request from a processor. The memory management device includes a coloring information storage unit that stores coloring information generated based on a data characteristic of write target data to be written into at least one of the nonvolatile semiconductor memory and the volatile semiconductor memory, and a writing management unit that references the coloring information to determines a region into which the write target data is written from the nonvolatile semiconductor memory and the volatile semiconductor memory.",
      "patent_id": null,
      "publication_number": "US10776007B2",
      "publication_date": "2020-09-15",
      "filing_date": "2015-11-11",
      "priority_date": "2009-07-17",
      "assignee": null,
      "inventor": null,
      "link": null,
      "pdf": "https://patentimages.storage.googleapis.com/67/62/4a/2f7b79163de8cc/US10776007.pdf"
    }
  ]
}