"""
Report Build Index - AI ë°˜ë„ì²´ ì‹œì¥ ë³´ê³ ì„œ RAG ì¸ë±ìŠ¤ ìƒì„±
"""
from __future__ import annotations

import os
from pathlib import Path
from typing import List, Dict, Any
import json

try:
    from langchain_community.document_loaders import PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_openai import OpenAIEmbeddings
    from langchain.schema import Document
    _HAS_LANGCHAIN = True
except ImportError:
    _HAS_LANGCHAIN = False
    print("âš ï¸ Install: pip install langchain langchain-community langchain-openai faiss-cpu pypdf")


def get_project_root() -> Path:
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸° (Windows/Linux í˜¸í™˜)"""
    current = Path(__file__).resolve()
    
    # rag/ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ëœ ê²½ìš°
    if current.parent.name == 'rag':
        return current.parent.parent
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ëœ ê²½ìš°
    return current.parent


class ReportIndexBuilder:
    """AI ë°˜ë„ì²´ ì‹œì¥ ë³´ê³ ì„œ RAG ì¸ë±ìŠ¤ ë¹Œë”"""
    
    def __init__(
        self,
        pdf_path: str = None,
        index_dir: str = None,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        if not _HAS_LANGCHAIN:
            raise ImportError("LangChain is required")
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸
        self.project_root = get_project_root()
        
        # PDF ê²½ë¡œ
        if pdf_path is None:
            pdf_path = self.project_root / "data" / "AIë°˜ë„ì²´ì‹œì¥í˜„í™©ë°ì „ë§.pdf"
        self.pdf_path = Path(pdf_path)
        
        # ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬
        if index_dir is None:
            index_dir = self.project_root / "rag" / "indexes"
        self.index_dir = Path(index_dir)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=os.environ.get("OPENAI_API_KEY")
        )
        
        self.vectorstore = None
        self.documents = []
    
    def load_pdf(self) -> List[Document]:
        """PDF ë¡œë“œ"""
        print(f"ğŸ“„ Loading PDF: {self.pdf_path}")
        print(f"   Absolute: {self.pdf_path.resolve()}")
        
        if not self.pdf_path.exists():
            raise FileNotFoundError(
                f"\nâŒ PDF not found!\n"
                f"Expected: {self.pdf_path}\n"
                f"Absolute: {self.pdf_path.resolve()}\n"
            )
        
        loader = PyPDFLoader(str(self.pdf_path))
        documents = loader.load()
        
        print(f"âœ… Loaded {len(documents)} pages")
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œ ë¶„í• """
        print(f"âœ‚ï¸ Splitting (size={self.chunk_size}, overlap={self.chunk_overlap})")
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        
        chunks = splitter.split_documents(documents)
        print(f"âœ… Created {len(chunks)} chunks")
        return chunks
    
    def build_index(self, force_rebuild: bool = False) -> FAISS:
        """ì¸ë±ìŠ¤ ìƒì„±/ë¡œë“œ"""
        index_path = self.index_dir / "ai_chip_market"
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
        if not force_rebuild and index_path.exists():
            print(f"ğŸ“¦ Loading from {index_path}")
            self.vectorstore = FAISS.load_local(
                str(index_path),
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print("âœ… Loaded")
            return self.vectorstore
        
        # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
        print("ğŸ—ï¸ Building new index...")
        documents = self.load_pdf()
        self.documents = self.split_documents(documents)
        
        print("ğŸ”¢ Creating embeddings...")
        self.vectorstore = FAISS.from_documents(self.documents, self.embeddings)
        
        print(f"ğŸ’¾ Saving to {index_path}")
        self.vectorstore.save_local(str(index_path))
        
        # ë©”íƒ€ë°ì´í„°
        metadata = {
            "pdf_path": str(self.pdf_path),
            "total_pages": len(documents),
            "total_chunks": len(self.documents),
            "chunk_size": self.chunk_size,
            "chunk_overlap": self.chunk_overlap
        }
        
        with open(self.index_dir / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print("âœ… Built and saved")
        return self.vectorstore


class ReportRAGRetriever:
    """Report Agentìš© ê²€ìƒ‰ê¸°"""
    
    def __init__(self, index_dir: str = None):
        if not _HAS_LANGCHAIN:
            raise ImportError("LangChain required")
        
        project_root = get_project_root()
        
        if index_dir is None:
            index_dir = project_root / "rag" / "indexes"
        
        self.index_dir = Path(index_dir)
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=os.environ.get("OPENAI_API_KEY")
        )
        
        index_path = self.index_dir / "ai_chip_market"
        if not index_path.exists():
            raise FileNotFoundError(f"Index not found: {index_path}")
        
        self.vectorstore = FAISS.load_local(
            str(index_path),
            self.embeddings,
            allow_dangerous_deserialization=True
        )
    
    def retrieve_industry_context(self, k: int = 3) -> str:
        queries = ["AI ë°˜ë„ì²´ ì‹œì¥ ê·œëª¨", "AI ë°˜ë„ì²´ ì‚°ì—… ë™í–¥"]
        results = []
        for q in queries:
            results.extend([d.page_content for d in self.vectorstore.similarity_search(q, k=k)])
        return "\n\n".join(list(dict.fromkeys(results))[:5])
    
    def retrieve_policy_context(self, k: int = 3) -> str:
        queries = ["ì£¼ìš”êµ­ AI ë°˜ë„ì²´ ì •ì±…", "ë°˜ë„ì²´ ì§€ì› ì •ì±…"]
        results = []
        for q in queries:
            results.extend([d.page_content for d in self.vectorstore.similarity_search(q, k=k)])
        return "\n\n".join(list(dict.fromkeys(results))[:5])
    
    def retrieve_korea_position(self, k: int = 3) -> str:
        queries = ["í•œêµ­ AI ë°˜ë„ì²´ ê²½ìŸë ¥", "í•œêµ­ ë°˜ë„ì²´ í˜„í™©"]
        results = []
        for q in queries:
            results.extend([d.page_content for d in self.vectorstore.similarity_search(q, k=k)])
        return "\n\n".join(list(dict.fromkeys(results))[:3])


def main():
    import sys
    from dotenv import load_dotenv
    
    load_dotenv()
    
    print("\n" + "="*80)
    print("ğŸ—ï¸ Report Index Builder")
    print("="*80)
    
    root = get_project_root()
    print(f"\nğŸ“ Project Root: {root}")
    
    pdf = root / "data" / "AIë°˜ë„ì²´ì‹œì¥í˜„í™©ë°ì „ë§.pdf"
    print(f"ğŸ“„ PDF: {pdf}")
    print(f"   Exists: {pdf.exists()}")
    
    if not pdf.exists():
        print(f"\nâŒ PDF not found!")
        print(f"   Expected: {pdf}")
        print(f"   Absolute: {pdf.resolve()}")
        print(f"\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print(f"   1. PDFë¥¼ ë‹¤ìŒ ìœ„ì¹˜ì— ë°°ì¹˜: {root / 'data'}")
        print(f"   2. íŒŒì¼ëª… í™•ì¸: AIë°˜ë„ì²´ì‹œì¥í˜„í™©ë°ì „ë§.pdf")
        
        data_dir = root / "data"
        if not data_dir.exists():
            print(f"\nğŸ”§ Creating: {data_dir}")
            data_dir.mkdir(parents=True, exist_ok=True)
        
        sys.exit(1)
    
    try:
        builder = ReportIndexBuilder()
        builder.build_index(force_rebuild=False)
        
        print("\n" + "="*80)
        print("ğŸ§ª Testing")
        print("="*80)
        
        retriever = ReportRAGRetriever()
        
        print("\n1ï¸âƒ£ Industry:")
        print(retriever.retrieve_industry_context()[:200] + "...")
        
        print("\n2ï¸âƒ£ Policy:")
        print(retriever.retrieve_policy_context()[:200] + "...")
        
        print("\n3ï¸âƒ£ Korea:")
        print(retriever.retrieve_korea_position()[:200] + "...")
        
        print("\n" + "="*80)
        print("âœ… Success!")
        print("="*80)
        print(f"ğŸ“¦ Index: {root / 'rag' / 'indexes' / 'ai_chip_market'}")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ Failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()