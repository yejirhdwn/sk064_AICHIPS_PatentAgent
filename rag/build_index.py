"""
Build Patent Market RAG Index (Chroma) - PDF ì§ì ‘ ë¡œë“œ (RAG í´ë” ê¸°ì¤€ ì‹¤í–‰)
í’ˆì§ˆ ê°œì„ :
- bge-m3 ì„ë² ë”©(ë‹¤êµ­ì–´ ê°•í•¨) ê¸°ë³¸ê°’
- í…ìŠ¤íŠ¸ ì •ê·œí™” í›„ ì²­í¬
- ìœ ì‚¬ë„ ì ìˆ˜(similarity_search_with_score) + ì¤‘ë³µ ì œê±°
- í˜ì´ì§€/ì²­í¬/ì ìˆ˜ ì¶œë ¥
"""
from __future__ import annotations

import os
import re
import unicodedata
from pathlib import Path
from typing import List, Tuple

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_chroma import Chroma
# âš ï¸ Deprecation ëŒ€ì‘: langchain_ollama ìš°ì„ , ì—†ìœ¼ë©´ communityë¡œ í´ë°±
try:
    from langchain_ollama import OllamaEmbeddings  # pip install langchain-ollama
except Exception:
    from langchain_community.embeddings import OllamaEmbeddings

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# -------------------------------------------------------------
# 1) PDF ê²½ë¡œ ìë™ ì •ê·œí™” (í•­ìƒ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
# -------------------------------------------------------------
def resolve_repo_path(p: str | Path) -> Path:
    p = Path(p)
    if p.is_absolute():
        return p
    repo_root = Path(__file__).resolve().parents[1]
    return (repo_root / p).resolve()


# -------------------------------------------------------------
# 2) í…ìŠ¤íŠ¸ ì •ê·œí™” ìœ í‹¸ (PDF ë…¸ì´ì¦ˆ ì œê±°)
# -------------------------------------------------------------
def normalize_text(t: str) -> str:
    if not t:
        return t
    t = unicodedata.normalize("NFKC", t)
    t = re.sub(r"-\s*\n\s*", "", t)      # í•˜ì´í”ˆ ì¤„ë°”ê¿ˆ ì—°ê²° ì œê±°
    t = t.replace("\r", "")
    t = re.sub(r"\s*\n\s*", " ", t)      # ê°œí–‰ â†’ ê³µë°±
    t = re.sub(r"[ \t]+", " ", t)        # ì¤‘ë³µ ê³µë°± ì¶•ì†Œ
    return t.strip()


# -------------------------------------------------------------
# 3) PDF ë¡œë“œ
# -------------------------------------------------------------
def load_pdf_document(pdf_path: str) -> List[Document]:
    documents: List[Document] = []
    pdf_file = Path(pdf_path)

    if not pdf_file.exists():
        print(f"âŒ PDF file not found: {pdf_file}")
        return documents

    print(f"ğŸ“„ Loading PDF: {pdf_file.name}")
    try:
        loader = PyPDFLoader(str(pdf_file))
        docs = loader.load()

        for doc in docs:
            doc.page_content = normalize_text(doc.page_content)
            doc.metadata["source"] = str(pdf_file)
            doc.metadata["filename"] = pdf_file.name
            doc.metadata["extension"] = ".pdf"
            if "page" not in doc.metadata:
                doc.metadata["page"] = None

        documents.extend(docs)
        print(f"  âœ… Loaded {len(docs)} pages from PDF")

    except Exception as e:
        print(f"  âš ï¸ Failed to load PDF: {e}")

    return documents


# -------------------------------------------------------------
# 4) ë¬¸ì„œ ë¶„í•  (+chunk_id, page ë³´ì¡´)
# -------------------------------------------------------------
def split_documents(documents: List[Document], chunk_size: int = 800, chunk_overlap: int = 250) -> List[Document]:
    if not documents:
        return []

    print(f"\nâœ‚ï¸ Splitting documents (chunk_size={chunk_size}, overlap={chunk_overlap})...")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " ", ""],
    )

    split_docs = splitter.split_documents(documents)

    for i, d in enumerate(split_docs):
        d.page_content = normalize_text(d.page_content)
        d.metadata["chunk_id"] = i
        if "page" not in d.metadata:
            d.metadata["page"] = None

    print(f"ğŸ“„ Total chunks: {len(split_docs)}")
    return split_docs


# -------------------------------------------------------------
# 5) ê²€ìƒ‰ ê²°ê³¼ ì¤‘ë³µ ì œê±°
# -------------------------------------------------------------
def dedupe_results(results: List[Tuple[Document, float]], max_items: int = 3) -> List[Tuple[Document, float]]:
    """ê°™ì€ í˜ì´ì§€Â·chunk ë˜ëŠ” ê±°ì˜ ë™ì¼í•œ ìŠ¤ë‹ˆí«ì€ ì œê±°"""
    seen = set()
    out: List[Tuple[Document, float]] = []
    for doc, score in results:
        page = doc.metadata.get("page")
        chunk_id = doc.metadata.get("chunk_id")
        key = (page, chunk_id)
        snippet_key = normalize_text(doc.page_content[:160])
        if key in seen or snippet_key in seen:
            continue
        seen.add(key)
        seen.add(snippet_key)
        out.append((doc, score))
        if len(out) >= max_items:
            break
    return out


# -------------------------------------------------------------
# 6) Chroma ì¸ë±ìŠ¤ êµ¬ì¶•
# -------------------------------------------------------------
def build_chroma_index(
    pdf_path: str = "data/í’ˆëª©ë³„ICTì‹œì¥ë™í–¥_AIë°˜ë„ì²´.pdf",
    collection_name: str = "patent_market_index",
    chroma_dir: str = "./chroma",
    embedding_model: str = "bge-m3",   # ë‹¤êµ­ì–´ ì„ë² ë”©
    chunk_size: int = 800,
    chunk_overlap: int = 250,
) -> None:

    print("=" * 80)
    print("ğŸš€ Starting Patent Market RAG Index Build")
    print("=" * 80)

    # âœ… ê²½ë¡œ ì •ê·œí™”
    pdf_path = str(resolve_repo_path(pdf_path))
    print(f"ğŸ“‚ Current working dir: {Path().resolve()}")
    print(f"ğŸ“„ Resolved PDF path:   {pdf_path}")

    # 1) PDF ë¡œë“œ
    documents = load_pdf_document(pdf_path)
    if not documents:
        print("\nâŒ No documents loaded. Please check the PDF path.")
        return

    # 2) ë¬¸ì„œ ë¶„í• 
    split_docs = split_documents(documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    if not split_docs:
        print("\nâŒ No chunks created.")
        return

    # 3) ì„ë² ë”© ëª¨ë¸
    print(f"\nğŸ¤– Initializing embedding model: {embedding_model}")
    embeddings = OllamaEmbeddings(model=embedding_model)

    # 4) Chroma ìƒì„±/ì´ˆê¸°í™”
    chroma_path = Path(chroma_dir)
    chroma_path.mkdir(parents=True, exist_ok=True)

    print(f"\nğŸ’¾ Creating Chroma vector store...")
    print(f"  Collection: {collection_name}")
    print(f"  Persist dir: {chroma_path.resolve()}")

    try:
        temp_vs = Chroma(
            collection_name=collection_name,
            persist_directory=str(chroma_path),
            embedding_function=embeddings,
        )
        temp_vs.delete_collection()
        print(f"  â™»ï¸ Deleted existing collection: {collection_name}")
    except Exception:
        pass

    vectorstore = Chroma.from_documents(
        documents=split_docs,
        embedding=embeddings,
        collection_name=collection_name,
        persist_directory=str(chroma_path),
    )

    print(f"\nâœ… Index build completed!")
    print(f"  Total chunks indexed: {len(split_docs)}")
    print(f"  Collection name: {collection_name}")
    print(f"  Persist directory: {chroma_path}")

    # 5) í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ (ìœ ì‚¬ë„ ì ìˆ˜ + ì¤‘ë³µ ì œê±°, í˜ì´ì§€/ì²­í¬/ì ìˆ˜ ì¶œë ¥)
    print(f"\nğŸ” Testing retrieval with sample query...")
    test_queries = [
        "HBM AI ë°˜ë„ì²´ ì‹œì¥ ê·œëª¨",
        "HBM ì±„íƒì´ ë°ì´í„°ì„¼í„° ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥",
    ]

    for q in test_queries:
        # ì ìˆ˜ í¬í•¨ ê²€ìƒ‰ (ë²„ì „ í˜¸í™˜ ì•ˆì •)
        raw_results = vectorstore.similarity_search_with_score(q, k=12)  # ë„“ê²Œ ë½‘ê³ 
        results = dedupe_results(raw_results, max_items=3)               # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 3ê°œ

        print(f"\nTop 3 results for '{q}':")
        if not results:
            print("  (no result)")
            continue

        for rank, (doc, score) in enumerate(results, 1):
            page0 = doc.metadata.get("page")
            page_disp = (page0 + 1) if isinstance(page0, int) else "?"
            chunk_id = doc.metadata.get("chunk_id", "?")
            src = doc.metadata.get("source", "N/A")
            preview = doc.page_content.replace("\n", " ")
            print(f"  [{rank}] (score={score:.4f}) p.{page_disp}  chunk#{chunk_id}")
            print(f"      {preview[:350]}...")
            print(f"      Source: {src}")

    print("\n" + "=" * 80)
    print("âœ… RAG Index Build Complete!")
    print("=" * 80)


# -------------------------------------------------------------
# 7) CLI ì‹¤í–‰
# -------------------------------------------------------------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Build Patent Market RAG Index from PDF (RAG í´ë” ê¸°ì¤€)")
    parser.add_argument("--pdf-path", type=str, default="data/í’ˆëª©ë³„ICTì‹œì¥ë™í–¥_AIë°˜ë„ì²´.pdf")
    parser.add_argument("--collection", type=str, default="patent_market_index")
    parser.add_argument("--chroma-dir", type=str, default="./chroma")
    parser.add_argument("--embedding-model", type=str, default="bge-m3")  # â† í•„ìš”ì‹œ ë³€ê²½
    parser.add_argument("--chunk-size", type=int, default=800)
    parser.add_argument("--chunk-overlap", type=int, default=250)

    args = parser.parse_args()

    build_chroma_index(
        pdf_path=args.pdf_path,
        collection_name=args.collection,
        chroma_dir=args.chroma_dir,
        embedding_model=args.embedding_model,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
    )
