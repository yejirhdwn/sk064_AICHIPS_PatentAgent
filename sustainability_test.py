"""
Sustainability Agent í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (LLM Judge ë²„ì „)
- í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
- suitability_agent_v2_llm_judge.py ì‚¬ìš©
"""

import json
import sys
from pathlib import Path

# agents í´ë”ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
agents_path = project_root / "agents"
sys.path.insert(0, str(agents_path))

try:
    from suitability_agent_v2_llm_judge import SustainabilityScoreAgent
    print("âœ… Using LLM Judge version (v2)")
except ImportError:
    print("âš ï¸ v2 not found, trying v1...")
    from suitability_agent import SustainabilityScoreAgent
    print("âœ… Using calculation-only version (v1)")


def test_direct_input(use_llm: bool = True):
    """ë°©ë²• 1: ì§ì ‘ ì ìˆ˜ ì…ë ¥í•˜ì—¬ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ ë°©ë²• 1: ì§ì ‘ ì ìˆ˜ ì…ë ¥")
    print("="*80)
    
    agent = SustainabilityScoreAgent(
        tech_name="NPU",
        use_llm_judge=use_llm
    )
    
    # ì˜ˆì‹œ ì ìˆ˜
    result = agent.calculate_sustainability(
        originality_score=0.92,  # ë…ì°½ì„± ë§¤ìš° ë†’ìŒ
        market_score=0.88,        # ì‹œì¥ì„± ìš°ìˆ˜
        market_size_score=0.35,   # ì„¸ë¶€ ì ìˆ˜ (ì˜µì…˜)
        growth_potential_score=0.28,
        commercialization_readiness=0.25
    )
    
    return result


def test_from_saved_files(use_llm: bool = True):
    """ë°©ë²• 2: ì €ì¥ëœ ê²°ê³¼ íŒŒì¼ì—ì„œ ì ìˆ˜ ë¡œë“œ"""
    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ ë°©ë²• 2: ì €ì¥ëœ ê²°ê³¼ íŒŒì¼ ì‚¬ìš©")
    print("="*80)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    project_root = Path(__file__).parent
    originality_dir = project_root / "output" / "originality"
    market_dir = project_root / "output" / "market_evaluation"
    
    # ìµœì‹  íŒŒì¼ ì°¾ê¸°
    originality_files = list(originality_dir.glob("*_originality.json")) if originality_dir.exists() else []
    market_files = list(market_dir.glob("market_eval_*.json")) if market_dir.exists() else []
    
    if not originality_files:
        print(f"âŒ Originality ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ê²½ë¡œ: {originality_dir}")
        print(f"   ë¨¼ì € patent_originality_agent.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return None
    
    if not market_files:
        print(f"âŒ Market ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ê²½ë¡œ: {market_dir}")
        print(f"   ë¨¼ì € market_size_growth_agent.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return None
    
    # ìµœì‹  íŒŒì¼ ì„ íƒ
    originality_file = max(originality_files, key=lambda p: p.stat().st_mtime)
    market_file = max(market_files, key=lambda p: p.stat().st_mtime)
    
    print(f"\nğŸ“‚ Loading files:")
    print(f"   Originality: {originality_file.name}")
    print(f"   Market: {market_file.name}")
    
    # íŒŒì¼ ë¡œë“œ
    with open(originality_file, 'r', encoding='utf-8') as f:
        originality_data = json.load(f)
    
    with open(market_file, 'r', encoding='utf-8') as f:
        market_data = json.load(f)
    
    # ì ìˆ˜ ì¶”ì¶œ
    originality_score = originality_data.get("originality_score", 0.0)
    market_score = market_data.get("market_score", 0.0)
    tech_name = market_data.get("tech_name", "Unknown")
    
    print(f"\nğŸ“Š Extracted Scores:")
    print(f"   Tech Name: {tech_name}")
    print(f"   Originality: {originality_score:.4f}")
    print(f"   Market: {market_score:.4f}")
    
    # Agent ì‹¤í–‰
    agent = SustainabilityScoreAgent(
        tech_name=tech_name,
        use_llm_judge=use_llm
    )
    
    result = agent.calculate_sustainability(
        originality_score=originality_score,
        market_score=market_score,
        market_size_score=market_data.get("market_size_score"),
        growth_potential_score=market_data.get("growth_potential_score"),
        commercialization_readiness=market_data.get("commercialization_readiness")
    )
    
    return result


def test_various_scenarios(use_llm: bool = True):
    """ë°©ë²• 3: ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ ë°©ë²• 3: ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤")
    print("="*80)
    
    scenarios = [
        {
            "name": "Së“±ê¸‰ - í˜ì‹ ì  ê¸°ìˆ ",
            "tech_name": "Quantum_AI",
            "originality": 0.95,
            "market": 0.90
        },
        {
            "name": "Aë“±ê¸‰ - ìš°ìˆ˜í•œ ê¸°ìˆ ",
            "tech_name": "Edge_AI",
            "originality": 0.88,
            "market": 0.75
        },
        {
            "name": "Bë“±ê¸‰ - ì–‘í˜¸í•œ ê¸°ìˆ ",
            "tech_name": "IoT_Sensor",
            "originality": 0.82,
            "market": 0.60
        },
        {
            "name": "ì• ë§¤í•œ ì¼€ì´ìŠ¤ - ê³ ë…ì°½ì„± ì €ì‹œì¥",
            "tech_name": "Niche_Tech",
            "originality": 0.92,
            "market": 0.35
        },
    ]
    
    results = []
    for scenario in scenarios:
        print(f"\n{'â”€'*80}")
        print(f"ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
        print(f"{'â”€'*80}")
        
        agent = SustainabilityScoreAgent(
            tech_name=scenario["tech_name"],
            use_llm_judge=use_llm
        )
        
        result = agent.calculate_sustainability(
            originality_score=scenario["originality"],
            market_score=scenario["market"]
        )
        results.append(result)
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ“Š ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    if use_llm:
        print(f"{'Tech Name':<20} {'Orig':>6} {'Market':>6} {'Calc':>5} {'LLM':>5} {'Invest':>10} {'Risk':>8}")
        print("â”€"*80)
        for r in results:
            llm_eval = r.get('llm_evaluation', {})
            calc_grade = r.get('calculated_grade', 'N/A')
            llm_grade = r.get('final_grade', calc_grade)
            invest = llm_eval.get('investment_recommendation', 'N/A')[:8]
            risk = llm_eval.get('risk_level', 'N/A')
            
            print(f"{r['tech_name']:<20} {r['originality_score']:>6.2f} {r['market_score']:>6.2f} "
                  f"{calc_grade:>5} {llm_grade:>5} {invest:>10} {risk:>8}")
    else:
        print(f"{'Tech Name':<20} {'Orig':>6} {'Market':>6} {'Score':>7} {'Grade':>6}")
        print("â”€"*80)
        for r in results:
            print(f"{r['tech_name']:<20} {r['originality_score']:>6.2f} {r['market_score']:>6.2f} "
                  f"{r['sustainability_score']:>7.4f} {r['sustainability_grade']:>6}")
    
    return results


def test_llm_vs_calculation():
    """ë°©ë²• 4: LLM vs ìˆ˜ì‹ ë¹„êµ"""
    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ ë°©ë²• 4: LLM Judge vs ìˆ˜ì‹ ë¹„êµ")
    print("="*80)
    
    # ì• ë§¤í•œ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        {"name": "ê³ ë…ì°½ì„±-ì €ì‹œì¥", "orig": 0.92, "market": 0.35},
        {"name": "ì €ë…ì°½ì„±-ê³ ì‹œì¥", "orig": 0.76, "market": 0.88},
        {"name": "ì¤‘ê°„-ì¤‘ê°„", "orig": 0.82, "market": 0.65},
    ]
    
    print(f"\n{'ì¼€ì´ìŠ¤':<20} {'ìˆ˜ì‹ë“±ê¸‰':>10} {'LLMë“±ê¸‰':>10} {'ì°¨ì´':>6} {'LLMì¶”ì²œ':>12}")
    print("â”€"*80)
    
    for case in test_cases:
        # ìˆ˜ì‹ë§Œ
        agent_calc = SustainabilityScoreAgent(
            tech_name=case["name"],
            use_llm_judge=False
        )
        result_calc = agent_calc.calculate_sustainability(
            originality_score=case["orig"],
            market_score=case["market"]
        )
        
        # LLM Judge
        agent_llm = SustainabilityScoreAgent(
            tech_name=case["name"],
            use_llm_judge=True
        )
        result_llm = agent_llm.calculate_sustainability(
            originality_score=case["orig"],
            market_score=case["market"]
        )
        
        calc_grade = result_calc["sustainability_grade"]
        llm_grade = result_llm["final_grade"]
        diff = "ë™ì¼" if calc_grade == llm_grade else "ë³€ê²½"
        llm_rec = result_llm.get("llm_evaluation", {}).get("investment_recommendation", "N/A")
        
        print(f"{case['name']:<20} {calc_grade:>10} {llm_grade:>10} {diff:>6} {llm_rec:>12}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Sustainability Agent í…ŒìŠ¤íŠ¸ (LLM Judge)")
    parser.add_argument(
        "--mode", 
        choices=["direct", "files", "scenarios", "compare", "all"],
        default="direct",
        help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ (ê¸°ë³¸: direct)"
    )
    parser.add_argument(
        "--no-llm",
        action="store_true",
        help="LLM Judge ë¹„í™œì„±í™” (ìˆ˜ì‹ë§Œ ì‚¬ìš©)"
    )
    args = parser.parse_args()
    
    use_llm = not args.no_llm
    
    if use_llm:
        print("\nğŸ¤– LLM Judge í™œì„±í™”")
    else:
        print("\nğŸ“Š ìˆ˜ì‹ ê³„ì‚°ë§Œ ì‚¬ìš© (LLM ë¹„í™œì„±í™”)")
    
    try:
        if args.mode in ["direct", "all"]:
            print("\nğŸ¯ ì‹¤í–‰ ì¤‘: ì§ì ‘ ì ìˆ˜ ì…ë ¥ í…ŒìŠ¤íŠ¸")
            test_direct_input(use_llm=use_llm)
        
        if args.mode in ["files", "all"]:
            print("\nğŸ¯ ì‹¤í–‰ ì¤‘: ì €ì¥ëœ íŒŒì¼ í…ŒìŠ¤íŠ¸")
            test_from_saved_files(use_llm=use_llm)
        
        if args.mode in ["scenarios", "all"]:
            print("\nğŸ¯ ì‹¤í–‰ ì¤‘: ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
            test_various_scenarios(use_llm=use_llm)
        
        if args.mode in ["compare", "all"]:
            print("\nğŸ¯ ì‹¤í–‰ ì¤‘: LLM vs ìˆ˜ì‹ ë¹„êµ")
            test_llm_vs_calculation()
        
        print("\n" + "="*80)
        print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()